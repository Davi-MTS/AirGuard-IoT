import os
import sys
import time
import math
import logging
import smtplib
import pymongo
import pandas as pd
import numpy as np
from urllib.parse import quote_plus
from datetime import datetime, timedelta
import configparser

from sklearn.ensemble import IsolationForest, RandomForestRegressor
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# ==============================================================================
# CONFIGURA√á√ïES
# ==============================================================================

config = configparser.ConfigParser()
config_path = os.path.join(os.path.dirname(__file__), "..", "..", "config.ini")
config.read(config_path, encoding="utf-8")

MONGO_USER = config.get("MongoAtlas", "user")
MONGO_PASS = config.get("MongoAtlas", "password")
MONGO_CLUSTER = config.get("MongoAtlas", "cluster")

user_encoded = quote_plus(MONGO_USER)
pass_encoded = quote_plus(MONGO_PASS)

CONFIG = {
    "MONGO_CONNECTION_URI": f"mongodb+srv://{user_encoded}:{pass_encoded}@{MONGO_CLUSTER}",
    "MONGO_DATABASE": config.get("MongoAtlas", "db_monitoramento", fallback="Monitoramento_do_Ar"),
    "MONGO_COLLECTION_RAW": config.get("MongoAtlas", "collection_sensores", fallback="Leituras_Sensores"),
    "MONGO_COLLECTION_ANALYTICS": config.get("MongoAtlas", "collection_analiticas", fallback="Leituras_Analiticas"),
    "SMTP_SERVER": config.get("SMTP", "server"),
    "SMTP_PORT": config.getint("SMTP", "port"),
    "EMAIL_SENDER": config.get("SMTP", "sender"),
    "EMAIL_PASSWORD": config.get("SMTP", "password"),
    "EMAIL_RECEIVER": [email.strip() for email in config.get("SMTP", "receivers").split(",")],
    "UPDATE_INTERVAL": config.getint("App", "update_interval", fallback=300),
    "RETRAIN_INTERVAL_MINUTES": config.getint("App", "retrain_interval_minutes", fallback=5),
    "EMAIL_COOLDOWN_MINUTES": config.getint("App", "email_cooldown_minutes", fallback=5),
}

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class AirQualityPipeline:
    def __init__(self):
        self.model = None
        self.last_retrain_time = datetime.min
        self.last_email_sent = datetime.min

        try:
            self.mongo_client = pymongo.MongoClient(CONFIG["MONGO_CONNECTION_URI"])
            self.mongo_client.admin.command("ping")
            self.db = self.mongo_client[CONFIG["MONGO_DATABASE"]]
            logger.info("‚úÖ Pipeline connected to MongoDB.")
        except Exception as e:
            logger.error(f"‚ùå Critical Mongo error: {e}")
            sys.exit(1)

    def get_data(self):
        """Read raw data generated by the simulator."""
        try:
            collection = self.db[CONFIG["MONGO_COLLECTION_RAW"]]

            cursor = collection.find({}, {"_id": 0}).sort("timestamp", -1).limit(10000)
            df = pd.DataFrame(list(cursor))

            if df.empty:
                return None

            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
            cols_num = ["temperatura", "humidade", "gases_ppm", "pm25"]
            for col in cols_num:
                df[col] = pd.to_numeric(df[col], errors="coerce")

            return df.dropna(subset=cols_num).sort_values("timestamp")
        except Exception as e:
            logger.error(f"Error reading data: {e}")
            return None

    def process_and_classify(self, df):
        """
        1. Create human-readable classification (Excellent/Good/Moderate/Poor)
        2. Compute unified vectorial air quality index
        3. Prepare temporal features
        """

        def classify_quality(row):
            pm, gas, temp = row["pm25"], row["gases_ppm"], row["temperatura"]
            if pm > 55 or gas > 800 or (temp < 14 or temp > 30):
                return "Ruim"
            elif (35 < pm <= 55) or (500 < gas <= 800):
                return "Moderada"
            elif (12 <= pm <= 35) or (300 <= gas <= 500):
                return "Boa"
            else:
                return "Excelente"

        def compute_vector_index(row):
            LIMITE_PM = 35.0
            LIMITE_GAS = 500.0
            norm_pm = row["pm25"] / LIMITE_PM
            norm_gas = row["gases_ppm"] / LIMITE_GAS
            indice = math.sqrt(norm_pm**2 + norm_gas**2) * 50.0
            return round(indice, 2)

        df = df.copy()
        df["classificacao_ar"] = df.apply(classify_quality, axis=1)
        df["carga_poluente"] = df.apply(compute_vector_index, axis=1)

        df["hora_do_dia"] = df["timestamp"].dt.hour
        df["dia_da_semana"] = df["timestamp"].dt.weekday

        return df

    def detect_anomalies(self, df):
        """
        Detect anomalies using a RandomForestRegressor as a baseline model.
        """
        if df.empty:
            return df

        features = ["temperatura", "humidade", "hora_do_dia", "dia_da_semana"]
        X = df[features]
        y = df["carga_poluente"]

        now = datetime.now()
        minutes_since_train = (now - self.last_retrain_time).total_seconds() / 60

        if self.model is None or minutes_since_train > CONFIG["RETRAIN_INTERVAL_MINUTES"]:
            logger.info(f"üß† Training anomaly model (last train: {int(minutes_since_train)} min ago)...")
            try:
                self.model = RandomForestRegressor(
                    n_estimators=50, max_depth=10, random_state=42, n_jobs=-1
                )
                self.model.fit(X, y)
                self.last_retrain_time = now
            except Exception as e:
                logger.error(f"Error training model: {e}")

        if self.model:
            df["carga_estimada"] = self.model.predict(X)
            df["desvio_modelo"] = df["carga_poluente"] - df["carga_estimada"]
        else:
            df["carga_estimada"] = df["carga_poluente"]
            df["desvio_modelo"] = 0

        def check_anomaly(row):
            reasons = []
            is_anomaly = False

            if row["desvio_modelo"] > 25.0:
                reasons.append(f"Polui√ß√£o acima do padr√£o para {row['hora_do_dia']}h")
                is_anomaly = True

            if row["carga_poluente"] > 100:
                reasons.append("√çndice Vetorial Cr√≠tico")
                is_anomaly = True
            elif row["pm25"] > 55:
                reasons.append("Poeira Alta")
                is_anomaly = True
            elif row["gases_ppm"] > 800:
                reasons.append("Gases Altos")
                is_anomaly = True

            tipo = ", ".join(reasons) if is_anomaly else "Normal"
            return is_anomaly, tipo

        result = df.apply(check_anomaly, axis=1)
        df["anomalia_detectada"] = result.apply(lambda x: x[0])
        df["tipo_anomalia"] = result.apply(lambda x: x[1])

        return df

    def save_to_mongo(self, df):
        """Persist processed DataFrame in analytics collection."""
        try:
            coll = self.db[CONFIG["MONGO_COLLECTION_ANALYTICS"]]
            coll.delete_many({})

            cols_to_drop = ["hora_do_dia", "dia_da_semana"]
            df_save = df.drop(columns=[c for c in cols_to_drop if c in df.columns])

            records = df_save.to_dict("records")
            if records:
                coll.insert_many(records)
                logger.info(f"üíæ Analytics: {len(records)} registros atualizados.")
        except Exception as e:
            logger.error(f"Error saving to Mongo: {e}")

    def send_alert_email(self, anomaly_records):
        """Send e-mail alert with cooldown to avoid spam."""
        critical = anomaly_records[anomaly_records["carga_poluente"] > 100]

        if critical.empty:
            return

        now = datetime.now()
        minutes_since_email = (now - self.last_email_sent).total_seconds() / 60

        if minutes_since_email < CONFIG["EMAIL_COOLDOWN_MINUTES"]:
            logger.info(
                "‚è≥ Critical alert detected but e-mail in cooldown "
                f"({int(minutes_since_email)}/{CONFIG['EMAIL_COOLDOWN_MINUTES']} min)."
            )
            return

        logger.info(f"üìß Sending alert for {len(critical)} critical records...")

        msg = MIMEMultipart()
        msg["From"] = CONFIG["EMAIL_SENDER"]
        msg["To"] = ", ".join(CONFIG["EMAIL_RECEIVER"])
        msg["Subject"] = f"üö® ALERTA AMBIENTAL - {now.strftime('%H:%M')}"

        html_table = critical[
            ["timestamp", "localizacao", "carga_poluente", "tipo_anomalia"]
        ].tail(5).to_html(index=False)

        body = f"""
        <h3>Alerta de Qualidade do Ar</h3>
        <p>O sistema detectou √≠ndices vetoriais acima do limite de seguran√ßa (100).</p>
        {html_table}
        <p><i>Este √© um alerta autom√°tico. Pr√≥ximo aviso somente em {CONFIG['EMAIL_COOLDOWN_MINUTES']} minutos se persistir.</i></p>
        """
        msg.attach(MIMEText(body, "html"))

        try:
            server = smtplib.SMTP(CONFIG["SMTP_SERVER"], CONFIG["SMTP_PORT"])
            server.starttls()
            server.login(CONFIG["EMAIL_SENDER"], CONFIG["EMAIL_PASSWORD"])
            server.sendmail(CONFIG["EMAIL_SENDER"], CONFIG["EMAIL_RECEIVER"], msg.as_string())
            server.quit()

            self.last_email_sent = now
            logger.info("‚úÖ E-mail enviado com sucesso.")
        except Exception as e:
            logger.error(f"Failed to send email: {e}")

    def run_cycle(self):
        df = self.get_data()
        if df is None:
            return

        df = self.process_and_classify(df)
        df = self.detect_anomalies(df)

        anomalies = df[df["anomalia_detectada"] == True]
        if not anomalies.empty:
            count = len(anomalies)
            logger.warning(f"‚ö†Ô∏è {count} anomalias detectadas no lote.")
            self.send_alert_email(anomalies)

        self.save_to_mongo(df)

    def start(self):
        logger.info("--- Optimized AI Pipeline Started ---")
        try:
            while True:
                self.run_cycle()
                logger.info(f"üí§ Sleeping for {CONFIG['UPDATE_INTERVAL']}s...")
                time.sleep(CONFIG["UPDATE_INTERVAL"])
        except KeyboardInterrupt:
            logger.info("Pipeline stopped.")


if __name__ == "__main__":
    pipeline = AirQualityPipeline()
    pipeline.start()

